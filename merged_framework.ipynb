{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PinKem253/Baseline_framework/blob/main/merged_framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bibWAyjooJQu"
      },
      "source": [
        "#Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syr4a4Hqc92D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4aee055-b789-4a48-8560-4b6289e43c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "\n",
        "import tensorflow as tf\n",
        "import sklearn as sk\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc,\n",
        "    confusion_matrix, classification_report, matthews_corrcoef, average_precision_score\n",
        ")\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        ")\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC, OneClassSVM\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Outlier Detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "# Advanced ML Models\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Imbalanced Data Handling\n",
        "!pip install imblearn\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "\n",
        "try:\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "except:\n",
        "  !pip install imblearn\n",
        "  from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEFs3jXQoWJK"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYQbU71Nr79a"
      },
      "outputs": [],
      "source": [
        "def OC_preprocess(datapath:str,\n",
        "                  train_size:float,\n",
        "                  ):\n",
        "\n",
        "  #Assume target_class as label\n",
        "  target_column = \"Label\"\n",
        "\n",
        "  #read data and rename last column\n",
        "  df = pd.read_csv(datapath)\n",
        "  df.rename(columns={df.columns[-1]: target_column},inplace=True)\n",
        "\n",
        "  #Clean data\n",
        "  df.drop_duplicates().reset_index(drop=True,inplace =True)\n",
        "  df.replace([-np.inf,np.inf],np.nan,inplace =True)\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  cate_col = [col for col in df.columns if df[col].dtype =='object' and col != target_column]\n",
        "  df.drop(columns=cate_col,inplace=True)\n",
        "\n",
        "  #Encoding for oneclass task\n",
        "  df[target_column] = df[target_column].apply(lambda x: x.upper())\n",
        "  df[target_column] = df[target_column].apply(lambda x:1 if \"BENIGN\" in str(x) else -1)\n",
        "\n",
        "  #Remove unimportant features\n",
        "  corr_threshold = 0.05\n",
        "  correlations = df.corr()\n",
        "  target_correlations = correlations.iloc[:-1,-1]\n",
        "  unimportant_features = target_correlations[abs(target_correlations) < corr_threshold].index\n",
        "  df = df.drop(columns=unimportant_features)\n",
        "\n",
        "  #separate trainset: 0 and testset 0 and 1\n",
        "  X = df.iloc[:,:-1]\n",
        "  y = df.iloc[:,-1]\n",
        "\n",
        "  X_train = df[df[target_column] == 1].iloc[:, :-1]\n",
        "  y_train = df[df[target_column] == 1][target_column]\n",
        "\n",
        "  X_test = df[df[target_column] == -1].iloc[:, :-1]\n",
        "  y_test = df[df[target_column] == -1][target_column]\n",
        "\n",
        "  X_train,X_benign_test,y_train,y_benign_test = train_test_split(X_train,y_train,random_state = 42,train_size= train_size,shuffle=True)\n",
        "  X_test = pd.concat([X_test,X_benign_test],axis=0).reset_index(drop=True)\n",
        "  y_test = pd.concat([y_test,y_benign_test],axis=0).reset_index(drop=True)\n",
        "\n",
        "  #Resampling data\n",
        "  #smote = SMOTE()\n",
        "  #X_train,y_train = smote.fit_resample(X_train,y_train)\n",
        "\n",
        "  #Scaling data\n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  return X_train,X_test,y_train,y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vRfyU8Ar-C1"
      },
      "outputs": [],
      "source": [
        "def MND_preprocess(datapath:str,\n",
        "                   train_size:float,\n",
        "                   ):\n",
        "  #Assume target_class as label\n",
        "  target_column = \"Label\"\n",
        "\n",
        "\n",
        "  #read data and rename last column\n",
        "  df = pd.read_csv(datapath)\n",
        "  df.rename(columns={df.columns[-1]: \"Label\"},inplace=True)\n",
        "\n",
        "  #Clean data\n",
        "  df.drop_duplicates().reset_index(drop=True,inplace=True)\n",
        "  df.replace([-np.inf,np.inf],np.nan,inplace=True)\n",
        "  df.dropna(inplace=True)\n",
        "\n",
        "  cate_col = [col for col in df.columns if df[col].dtype =='object' and col != target_column]\n",
        "  df.drop(columns=cate_col,inplace=True)\n",
        "\n",
        "  #Encode Benign as 0, attack from 1 onwards\n",
        "  df[target_column] = df[target_column].apply(lambda x: x.upper())\n",
        "  df[target_column] = df[target_column].apply(lambda x:0 if \"BENIGN\" in str(x) else x)\n",
        "\n",
        "  benign = df[df[target_column] == 0]\n",
        "  attack = df[df[target_column] != 0]\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "  attack[target_column] = label_encoder.fit_transform(attack[target_column])\n",
        "  attack[target_column] += 1\n",
        "\n",
        "  df = pd.concat([benign,attack])\n",
        "\n",
        "  #Remove unimportant features\n",
        "  corr_threshold = 0.05\n",
        "  correlations = df.corr()\n",
        "  target_correlations = correlations.iloc[:-1,-1]\n",
        "  unimportant_features = target_correlations[abs(target_correlations) < corr_threshold].index\n",
        "  df = df.drop(columns=unimportant_features)\n",
        "\n",
        "  #Separate train and test\n",
        "  X=df.iloc[:,:-1]\n",
        "  y=df.iloc[:,-1]\n",
        "\n",
        "  X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,train_size=train_size,shuffle=True)\n",
        "\n",
        "  #Resampling\n",
        "  #y_train = y_train.astype(int)\n",
        "  #smote = SMOTE()\n",
        "  #X_train,y_train = smote.fit_resample(X_train,y_train)\n",
        "\n",
        "  #Scale\n",
        "  #scaler = StandardScaler()\n",
        "  #X_train = scaler.fit_transform(X_train)\n",
        "  #X_test = scaler.transform(X_test)\n",
        "\n",
        "  return X_train,X_test,y_train,y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vblGZ3gm8yAu"
      },
      "source": [
        "#Evaluation and get models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation_metrics(model, X_test, y_test, y_pred):\n",
        "\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, average=\"macro\"),\n",
        "        'Recall': recall_score(y_test, y_pred, average=\"macro\"),\n",
        "        'F1': f1_score(y_test, y_pred, average=\"macro\"),\n",
        "        'MCC': matthews_corrcoef(y_test, y_pred),\n",
        "    }\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "LRJ9aXkV3f-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcIAi5xV-ze5"
      },
      "outputs": [],
      "source": [
        "def get_model(model_name):\n",
        "  model_dict = {\n",
        "      #OneClass task\n",
        "      \"oneclass_svm\": OneClassSVM(),\n",
        "      \"iso_forest\": IsolationForest(),\n",
        "      \"lof\": LocalOutlierFactor(novelty=True),\n",
        "      \"robust_covariance\": EllipticEnvelope(),\n",
        "      \"gaussian_mixture\": GaussianMixture(),\n",
        "\n",
        "      #MND\n",
        "      \"random_forest\": RandomForestClassifier(),\n",
        "      \"svm\": SVC(),\n",
        "      \"knn\": KNeighborsClassifier(),\n",
        "      \"xgb\": XGBClassifier(),\n",
        "      \"lgb\": LGBMClassifier(),\n",
        "      \"logistic_regression\": LogisticRegression(),\n",
        "      \"decision_tree\": DecisionTreeClassifier(),\n",
        "      \"gradient_boost\": GradientBoostingClassifier(),\n",
        "      \"naive_bayes\": GaussianNB(),\n",
        "  }\n",
        "\n",
        "  if model_name in model_dict:\n",
        "    return model_dict[model_name]\n",
        "  else:\n",
        "    raise ValueError(f\"{model_name} not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0jOrooVB62q"
      },
      "source": [
        "#Training function for each task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uwqIVsusAB7"
      },
      "outputs": [],
      "source": [
        "def oneclass_train(dataset:Dict,\n",
        "                  train_size:float,\n",
        "                  models:List,\n",
        "                  output_result:str,\n",
        "                  task: str,\n",
        "                  ):\n",
        "  for dataset_title,dataset_path in dataset.items():\n",
        "    results =[]\n",
        "\n",
        "    #Benign:1 and Attack:-1\n",
        "    X_train,X_test,y_train,y_test = OC_preprocess(dataset_path,train_size)\n",
        "\n",
        "    for model_name in models:\n",
        "      model = get_model(model_name)\n",
        "      print(\"-\"*100)\n",
        "      print(f\"Start training using model {model_name}\")\n",
        "\n",
        "\n",
        "      start_train = time.time()\n",
        "      model.fit(X_train,y_train)\n",
        "      end_train = time.time()\n",
        "      time_train = end_train - start_train\n",
        "\n",
        "      start_test = time.time()\n",
        "      y_pred = model.predict(X_test)\n",
        "\n",
        "      end_test = time.time()\n",
        "      time_test = end_test - start_test\n",
        "      print(\"End training\")\n",
        "\n",
        "      metrics = evaluation_metrics(model,X_test,y_test,y_pred)\n",
        "      metrics.update({\n",
        "        \"Dataset\": dataset_title,\n",
        "        \"Task\": task,\n",
        "        \"Model\": model_name,\n",
        "        \"Time train\": time_train,\n",
        "        \"Time test\": time_test,\n",
        "      })\n",
        "\n",
        "      results.append(metrics)\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(df_results)\n",
        "\n",
        "    if not os.path.exists(output_result):\n",
        "      df_results.to_csv(output_result, index=False)\n",
        "    else:\n",
        "      df_results.to_csv(output_result, mode='a', index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aLQG1W_sATH"
      },
      "outputs": [],
      "source": [
        "def multiclass_novelty(dataset: Dict,\n",
        "                      train_size: float,\n",
        "                      models: List,\n",
        "                      output_result: str,\n",
        "                      task: str,\n",
        "                      ):\n",
        "\n",
        "  for dataset_title,dataset_path in dataset.items():\n",
        "    X_train,X_test,y_train,y_test = MND_preprocess(dataset_path,train_size)\n",
        "\n",
        "    for attack in range(1,len(y_train.unique())):\n",
        "      results = []\n",
        "      print(\"-\"*100)\n",
        "\n",
        "      #exclude attack from trainset\n",
        "      X_train_fit = X_train[y_train != attack]\n",
        "      y_train_fit = y_train[y_train != attack]\n",
        "\n",
        "      #mark excluded attack as -1\n",
        "      y_valid = y_test.copy()\n",
        "      y_valid[y_valid == attack] = -1\n",
        "      X_valid = X_test.loc[y_valid.index]\n",
        "\n",
        "      #Resampling\n",
        "      y_train_fit = y_train_fit.astype(int)\n",
        "      smote = SMOTE()\n",
        "      X_train_fit,y_train_fit = smote.fit_resample(X_train_fit,y_train_fit)\n",
        "\n",
        "      #Scale data\n",
        "      scaler = StandardScaler()\n",
        "      X_train_fit = scaler.fit_transform(X_train_fit)\n",
        "      X_valid = scaler.transform(X_valid)\n",
        "\n",
        "      for model_name in models:\n",
        "        model = get_model(model_name)\n",
        "        print(f\"\\nStart training with class {attack} dropped using model {model_name}\")\n",
        "\n",
        "        start_train = time.time()\n",
        "        model.fit(X_train_fit,y_train_fit)\n",
        "        end_train = time.time()\n",
        "        time_train = end_train - start_train\n",
        "\n",
        "        start_test = time.time()\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        end_test = time.time()\n",
        "        time_test = end_test - start_test\n",
        "        print(\"End training\")\n",
        "\n",
        "        y_valid = y_valid.astype(int)\n",
        "        y_pred = y_pred.astype(int)\n",
        "\n",
        "        metrics = evaluation_metrics(model,X_valid,y_valid,y_pred)\n",
        "        metrics.update({\n",
        "          \"Dataset\": dataset_title,\n",
        "          \"Task\": task,\n",
        "          \"Model\": model_name,\n",
        "          \"Time train\": time_train,\n",
        "          \"Time test\": time_test,\n",
        "        })\n",
        "        results.append(metrics)\n",
        "\n",
        "      df_results = pd.DataFrame(results)\n",
        "      print(df_results)\n",
        "\n",
        "      if not os.path.exists(output_result):\n",
        "        df_results.to_csv(output_result, index=False)\n",
        "      else:\n",
        "        df_results.to_csv(output_result, mode='a', index=False, header=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "755Q64EKsAfP"
      },
      "outputs": [],
      "source": [
        "def multiclass_q_novelty(dataset:str,\n",
        "                        train_size:float,\n",
        "                        models: List,\n",
        "                        output_result: str,\n",
        "                        task: str,\n",
        "                        ):\n",
        "  loop_time = int(input(f\"Number of experiments: \"))\n",
        "  remove_number = int(input(f\"Q classes to remove each run: \"))\n",
        "\n",
        "  for i in range(1,loop_time+1):\n",
        "    results = []\n",
        "    print(\"-\"*100)\n",
        "    print(f\"Running {i} time\")\n",
        "\n",
        "    for dataset_title,dataset_path in dataset.items():\n",
        "      X_train,X_test,y_train,y_test = MND_preprocess(dataset_path,train_size)\n",
        "\n",
        "      #Generate a random Q label list to remove from training\n",
        "      remove_list = np.random.choice(range(1,len(y_train.unique())),size=remove_number,replace=False).tolist()\n",
        "      print(f\"Remove {len(remove_list)} class {remove_list} in {i} time\")\n",
        "\n",
        "      #Remove all selected attack from training\n",
        "      X_train_fit = X_train[~y_train.isin(remove_list)]\n",
        "      y_train_fit = y_train[~y_train.isin(remove_list)]\n",
        "\n",
        "\n",
        "      #Mark selected remove list as -1\n",
        "      y_valid = y_test.copy()\n",
        "      y_valid[y_valid.isin(remove_list)] = -1\n",
        "      X_valid = X_test[y_valid]\n",
        "\n",
        "      y_train = y_train.astype(int)\n",
        "      smote = SMOTE()\n",
        "      X_train_fit,y_train_fit = smote.fit_resample(X_train_fit,y_train_fit)\n",
        "\n",
        "      scaler = StandardScaler()\n",
        "      X_train_fit = scaler.fit_transform(X_train_fit)\n",
        "      X_valid = scaler.transform(X_valid)\n",
        "\n",
        "      for model_name in models:\n",
        "        model = get_model(model_name)\n",
        "\n",
        "        print(f\"\\nStart fitting using model {model_name}\")\n",
        "        start_train = time.time()\n",
        "        model.fit(X_train_fit,y_train_fit)\n",
        "        end_train = time.time()\n",
        "        time_train = end_train - start_train\n",
        "\n",
        "        start_test = time.time()\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        end_test = time.time()\n",
        "        time_test = end_test - start_test\n",
        "        print(f\"End testing \\n\")\n",
        "\n",
        "        metrics = evaluation_metrics(model,X_valid,y_valid.astype(str),y_pred.astype(str))\n",
        "        metrics.update({\n",
        "          \"Dataset\": dataset_title,\n",
        "          \"Task\": task,\n",
        "          \"Model\": model_name,\n",
        "          \"Time train\": time_train,\n",
        "          \"Time test\": time_test,\n",
        "        })\n",
        "        results.append(metrics)\n",
        "\n",
        "      df_results = pd.DataFrame(results)\n",
        "      print(df_results)\n",
        "\n",
        "      if not os.path.exists(output_result):\n",
        "        df_results.to_csv(output_result, index=False)\n",
        "      else:\n",
        "        df_results.to_csv(output_result, mode='a', index=False, header=False)\n",
        "\n",
        "    print(f\"End training {i} time\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zcmfX0WGJC_"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCo0NuYosBFX"
      },
      "outputs": [],
      "source": [
        "def run_experiment(datasets:Dict,\n",
        "                   train_size:float,\n",
        "                   task:str,\n",
        "                   output_result:str\n",
        "                   ):\n",
        "\n",
        "  oneclass_models =[\n",
        "      \"oneclass_svm\",\n",
        "      \"iso_forest\",\n",
        "      \"lof\",\n",
        "      \"robust_covariance\",\n",
        "      #\"gaussian_mixture\",\n",
        "  ]\n",
        "  MND_models = [\n",
        "      \"random_forest\",\n",
        "      \"svm\",\n",
        "      \"knn\",\n",
        "      \"xgb\",\n",
        "      \"lgb\",\n",
        "      \"logistic_regression\",\n",
        "      \"decision_tree\",\n",
        "      \"gradient_boost\",\n",
        "      \"naive_bayes\",\n",
        "  ]\n",
        "  if(task==\"one class\"):\n",
        "    return oneclass_train(datasets,train_size,oneclass_models,output_result,task)\n",
        "\n",
        "  elif(task==\"multiclass novelty\"):\n",
        "    multiclass_novelty(datasets,train_size,MND_models,output_result,task)\n",
        "\n",
        "\n",
        "  elif task == \"multiclass q novelty\":\n",
        "    multiclass_q_novelty(datasets,train_size,MND_models,output_result,task)\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid task\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_8X_iMty9m7"
      },
      "source": [
        "#Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3uz7gIRzYzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8d7335-6c64-49d9-a6b7-9d21458171b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Start training with class 1 dropped using model random_forest\n"
          ]
        }
      ],
      "source": [
        "datasets ={\n",
        "    #\"CIC IOT\": \"/content/drive/MyDrive/Colab Notebooks/CIC IoT dataset 2023.csv\",\n",
        "    #\"Attack Type\": \"/content/drive/MyDrive/Colab Notebooks/AttackType.csv\",\n",
        "    \"IOT Intrusion\": \"/content/drive/MyDrive/Colab Notebooks/IoT_Intrusion.csv\",\n",
        "}\n",
        "output_result = \"/content/drive/MyDrive/Colab Notebooks/merge_result_mmb.csv\"\n",
        "\n",
        "\n",
        "train_size = 0.8\n",
        "task = \"multiclass novelty\"\n",
        "run_experiment(datasets,train_size,task,output_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZwzL5fEfv7lR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}